# ETL Data Modeling with Postgres

## Introduction
This project demonstrates the concept of data modeling with PostgreSQL, which reinforces the basis for building an ETL data pipeline uising Python.
A startup wants to analyse the data collected on songs and user activity on their new music streaming app. The collected data resides in a directory of JSON formatted files. The analytics team is particularly interested in understanding what songs users are listening to based on the data.

As a data engineer tasked to create a Postgres database with tables designed to optimize queries on song play analysis.

 Goals for this project are:
 1. Create a database schema
 2. Create an ETL pipeline for the analysis
 2. Test the created database and ETL pipeline running sample queries given by the anayltics team.


## The Data Sets
The Songs Dataset is a subset of [Million Song Dataset](http://millionsongdataset.com/)

     {"num_songs": 1, "artist_id": "ARD7TVE1187B99BFB1", "artist_latitude": null, "artist_longitude": null, "artist_location": "California - LA", "artist_name": "Casual", "song_id": "SOQLGFP12A58A7800E", "title": "OAKtown", "duration": 259.44771, "year": 0}
    
The Logs Dataset is generated by [Event Simulator](https://github.com/Interana/eventsim)
      
     {"artist":null,"auth":"Logged Out","firstName":null,"gender":null,"itemInSession":0,"lastName":null,"length":null,"level":"free","location":null,"method":"PUT","page":"Login","registration":null,"sessionId":52,"song":null,"status":307,"ts":1541207073796,"userAgent":null,"userId":""}

## Tools
Here is a brief overview of tools utilized in completing this project

`Python` as the main programming language used to analyse and also write the etl pipeline.

`SQL` as the main query language to run queries against the Sparkify Database.

`Jupyter Notebook` used as the initial enviroment to read, analyse and write out logic of the etl.

`VS Code IDE` as my preferref IDE to work with, was used to run the `etl.py`


## ETL

### Database Schema Design
The data modeling technique adopted for the Sparkify schema is the Star Schema which is a dimensional modeling. The purpose is to optimize the database for faster retrieval of data, which suit our the goal of the project. The Schema consists of one `Fact Table` which is the songplays table and fourdimension tables(dim_users, dim_songs, dim_artists and dim_time).

Below is the Star Schema Design

![SparkifyDB Schema Design](https://user-images.githubusercontent.com/24456790/224547926-0af1253c-ec1e-4032-8482-ebc6cafd05cc.png)


### Data Pipeline
 With are data in two local directories and based on the designed Schema, the etl pipeline is built. The ETL pipeline reads the json-formatted data(`song_data` and `log_data`) from the directories, processes and loads them into corresponding tables in the Spakify Database in PostgreSQL. The ELT completely run using Python and SQL.
 
 How To Run The ETL Pipeline
 
      # Run the sql_queries first to create the database and tables in Postgres
         python sql_queries.py
         
      # Run the etl.py to process and load the data to respective tables in the database.
         python etl.py
      

## Resources
[Psycopg](https://www.psycopg.org/docs/)

[PostgreSQL Tutorial](https://www.postgresqltutorial.com/)

[Pandas Docs](https://pandas.pydata.org/pandas-docs/stable/)



