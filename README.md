# ETL Data Modeling with Postgres

## Introduction
This project demonstrates the concept of data modeling with PostgreSQL, which reinforces the basis for building an ETL data pipeline uising Python.
A startup wants to analyse the data collected on songs and user activity on their new music streaming app. The collected data resides in a directory of JSON formatted files. The analytics team is particularly interested in understanding what songs users are listening to based on the data.

As a data engineer tasked to create a Postgres database with tables designed to optimize queries on song play analysis.

 Goals for this project are:
 1. Create a database schema
 2. Create an ETL pipeline for the analysis
 2. Test the created database and ETL pipeline running sample queries given by the anayltics team.


## The Data Sets
The Songs Dataset is a subset of [Million Song Dataset](http://millionsongdataset.com/)

     {"num_songs": 1, "artist_id": "ARD7TVE1187B99BFB1", "artist_latitude": null, "artist_longitude": null, "artist_location": "California - LA", "artist_name": "Casual", "song_id": "SOQLGFP12A58A7800E", "title": "OAKtown", "duration": 259.44771, "year": 0}
    
The Logs Dataset is generated by [Event Simulator](https://github.com/Interana/eventsim)
      
     {"artist":null,"auth":"Logged Out","firstName":null,"gender":null,"itemInSession":0,"lastName":null,"length":null,"level":"free","location":null,"method":"PUT","page":"Login","registration":null,"sessionId":52,"song":null,"status":307,"ts":1541207073796,"userAgent":null,"userId":""}





## Tools


## ETL - Batch Processing



## Resources



